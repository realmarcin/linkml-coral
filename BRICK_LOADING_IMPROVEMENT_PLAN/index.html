
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://realmarcin.github.io/linkml-coral/BRICK_LOADING_IMPROVEMENT_PLAN/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.19">
    
    
      
        <title>CDM Brick Loading Improvement Plan - linkml-coral</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cdm-brick-loading-improvement-plan" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="linkml-coral" class="md-header__button md-logo" aria-label="linkml-coral" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            linkml-coral
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              CDM Brick Loading Improvement Plan
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/realmarcin/linkml-coral" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="linkml-coral" class="md-nav__button md-logo" aria-label="linkml-coral" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    linkml-coral
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/realmarcin/linkml-coral" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../elements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schema
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cdm_store_quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CDM Store Quick Start
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#phase-1-critical-fixes-p0-fix-oom" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 1: Critical Fixes (P0 - Fix OOM)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Critical Fixes (P0 - Fix OOM)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-11-implement-chunked-parquet-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1.1: Implement Chunked Parquet Loading
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Task 1.1: Implement Chunked Parquet Loading">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-add-chunked-reader-function" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.1: Add chunked reader function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-add-chunked-collection-loader" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.2: Add chunked collection loader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-update-main-loading-function" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.3: Update main loading function
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-12-update-just-commands-with-safe-defaults" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1.2: Update Just Commands with Safe Defaults
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Task 1.2: Update Just Commands with Safe Defaults">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-update-load-cdm-store-bricks-lines-284-295" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.1: Update load-cdm-store-bricks (lines 284-295)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-add-new-load-cdm-store-bricks-full-command" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.2: Add new load-cdm-store-bricks-full command
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123-update-load-cdm-store-sample-lines-269-280" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.3: Update load-cdm-store-sample (lines 269-280)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#phase-2-user-experience-p1-warnings-progress" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 2: User Experience (P1 - Warnings &amp; Progress)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: User Experience (P1 - Warnings &amp; Progress)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-21-add-memory-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Task 2.1: Add Memory Monitoring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#task-22-add-progress-bars" class="md-nav__link">
    <span class="md-ellipsis">
      Task 2.2: Add Progress Bars
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#phase-3-performance-optimization-p2-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 3: Performance Optimization (P2 - Optional)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 3: Performance Optimization (P2 - Optional)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-31-direct-duckdb-import-advanced" class="md-nav__link">
    <span class="md-ellipsis">
      Task 3.1: Direct DuckDB Import (Advanced)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#testing-plan" class="md-nav__link">
    <span class="md-ellipsis">
      Testing Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Testing Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unit-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Unit Tests
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#integration-tests" class="md-nav__link">
    <span class="md-ellipsis">
      Integration Tests
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Benchmarks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rollout-plan" class="md-nav__link">
    <span class="md-ellipsis">
      Rollout Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Rollout Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#week-1-critical-fixes-p0" class="md-nav__link">
    <span class="md-ellipsis">
      Week 1: Critical Fixes (P0)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-2-user-experience-p1" class="md-nav__link">
    <span class="md-ellipsis">
      Week 2: User Experience (P1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#week-3-optional-performance-p2" class="md-nav__link">
    <span class="md-ellipsis">
      Week 3+: Optional Performance (P2)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#success-criteria" class="md-nav__link">
    <span class="md-ellipsis">
      Success Criteria
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Success Criteria">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#must-have-p0" class="md-nav__link">
    <span class="md-ellipsis">
      Must Have (P0)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#should-have-p1" class="md-nav__link">
    <span class="md-ellipsis">
      Should Have (P1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nice-to-have-p2" class="md-nav__link">
    <span class="md-ellipsis">
      Nice to Have (P2)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#risk-mitigation" class="md-nav__link">
    <span class="md-ellipsis">
      Risk Mitigation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Risk Mitigation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#risk-1-chunked-loading-is-slower" class="md-nav__link">
    <span class="md-ellipsis">
      Risk 1: Chunked loading is slower
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#risk-2-breaking-changes-for-existing-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Risk 2: Breaking changes for existing scripts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#risk-3-linkml-store-compatibility-issues" class="md-nav__link">
    <span class="md-ellipsis">
      Risk 3: linkml-store compatibility issues
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#documentation-updates" class="md-nav__link">
    <span class="md-ellipsis">
      Documentation Updates
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Documentation Updates">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#files-to-update" class="md-nav__link">
    <span class="md-ellipsis">
      Files to Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-code-review-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix: Code Review Checklist
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="cdm-brick-loading-improvement-plan">CDM Brick Loading Improvement Plan</h1>
<h2 id="overview">Overview</h2>
<p>This document outlines the implementation plan to fix out-of-memory (OOM) issues when loading CDM brick tables on machines with 64GB RAM.</p>
<p><strong>Problem</strong>: Loading ddt_brick0000476 (320M rows, 383 MB compressed) causes OOM due to loading entire file into memory.</p>
<p><strong>Solution</strong>: Implement chunked loading with memory monitoring and safe defaults.</p>
<p><strong>Timeline</strong>: 1-2 days development + 0.5 day testing</p>
<hr />
<h2 id="phase-1-critical-fixes-p0-fix-oom">Phase 1: Critical Fixes (P0 - Fix OOM)</h2>
<h3 id="task-11-implement-chunked-parquet-loading">Task 1.1: Implement Chunked Parquet Loading</h3>
<p><strong>File</strong>: <code>scripts/cdm_analysis/load_cdm_parquet_to_store.py</code></p>
<p><strong>Changes Required</strong>:</p>
<h4 id="111-add-chunked-reader-function">1.1.1: Add chunked reader function</h4>
<p><strong>Location</strong>: After line 187 (after <code>read_parquet_data</code>)</p>
<pre><code class="language-python">def read_parquet_chunked(
    parquet_path: Path,
    chunk_size: int = 100_000,
    max_rows: Optional[int] = None,
    verbose: bool = False
) -&gt; Iterator[pd.DataFrame]:
    &quot;&quot;&quot;
    Read parquet file in chunks to avoid loading entire file into memory.

    Args:
        parquet_path: Path to parquet file or directory (Delta Lake)
        chunk_size: Number of rows per chunk (default: 100K)
        max_rows: Maximum total rows to read (None = all)
        verbose: Print chunk progress

    Yields:
        DataFrame chunks
    &quot;&quot;&quot;
    import pyarrow.parquet as pq

    if parquet_path.is_dir():
        # Delta Lake format - read all parquet files in directory
        parquet_files = sorted([f for f in parquet_path.glob(&quot;*.parquet&quot;)
                               if not f.parent.name.startswith('_')])
        if not parquet_files:
            raise ValueError(f&quot;No parquet files found in {parquet_path}&quot;)
    else:
        # Single parquet file
        parquet_files = [parquet_path]

    total_yielded = 0
    for file_idx, pf in enumerate(parquet_files, 1):
        if verbose and len(parquet_files) &gt; 1:
            print(f&quot;    Reading file {file_idx}/{len(parquet_files)}: {pf.name}&quot;)

        parquet_file = pq.ParquetFile(pf)

        # Iterate over row groups in batches
        for batch in parquet_file.iter_batches(batch_size=chunk_size):
            df_chunk = batch.to_pandas()

            # Apply max_rows limit
            if max_rows is not None:
                rows_remaining = max_rows - total_yielded
                if rows_remaining &lt;= 0:
                    return
                if len(df_chunk) &gt; rows_remaining:
                    yield df_chunk.iloc[:rows_remaining]
                    return

            total_yielded += len(df_chunk)
            yield df_chunk
</code></pre>
<p><strong>Testing</strong>:</p>
<pre><code class="language-python"># Test chunked reading
chunk_gen = read_parquet_chunked(
    Path(&quot;data/enigma_coral.db/ddt_brick0000476&quot;),
    chunk_size=100_000,
    max_rows=500_000,
    verbose=True
)

total = 0
for chunk in chunk_gen:
    total += len(chunk)
    print(f&quot;Chunk: {len(chunk):,} rows, Total: {total:,}&quot;)
</code></pre>
<h4 id="112-add-chunked-collection-loader">1.1.2: Add chunked collection loader</h4>
<p><strong>Location</strong>: After line 406 (after <code>load_parquet_collection</code>)</p>
<pre><code class="language-python">def load_parquet_collection_chunked(
    parquet_path: Path,
    class_name: str,
    db,
    schema_view: SchemaView,
    max_rows: Optional[int] = None,
    chunk_size: int = 100_000,
    verbose: bool = False
) -&gt; int:
    &quot;&quot;&quot;
    Load a parquet table into linkml-store using chunked reading.

    This method loads data in chunks to avoid memory issues with large files.

    Args:
        parquet_path: Path to parquet file/directory
        class_name: LinkML class name for this data
        db: Database connection
        schema_view: SchemaView instance
        max_rows: Maximum rows to load (None = all)
        chunk_size: Rows per chunk (default: 100K)
        verbose: Print detailed progress

    Returns:
        Number of records loaded
    &quot;&quot;&quot;
    table_name = parquet_path.name
    print(f&quot;\nüì• Loading {table_name} as {class_name} (CHUNKED MODE)...&quot;)

    # Get total row count
    try:
        total_rows = get_parquet_row_count(parquet_path)
        load_rows = min(max_rows, total_rows) if max_rows else total_rows

        if max_rows and max_rows &lt; total_rows:
            print(f&quot;  üìä Total rows: {total_rows:,} (loading: {load_rows:,})&quot;)
        else:
            print(f&quot;  üìä Total rows: {total_rows:,}&quot;)

        # Estimate chunks
        num_chunks = (load_rows + chunk_size - 1) // chunk_size
        print(f&quot;  üì¶ Processing {num_chunks:,} chunks ({chunk_size:,} rows/chunk)&quot;)

    except Exception as e:
        print(f&quot;  ‚ö†Ô∏è  Could not get row count: {e}&quot;)
        total_rows = None
        num_chunks = None

    # Create or get collection
    collection_name = class_name
    try:
        collection = db.get_collection(collection_name)
        if verbose:
            print(f&quot;  üì¶ Using existing collection: {collection_name}&quot;)
    except:
        collection = db.create_collection(collection_name)
        if verbose:
            print(f&quot;  ‚ú® Created new collection: {collection_name}&quot;)

    # Load data in chunks
    start_time = time.time()
    total_loaded = 0
    chunk_num = 0

    try:
        chunk_generator = read_parquet_chunked(
            parquet_path,
            chunk_size=chunk_size,
            max_rows=max_rows,
            verbose=verbose
        )

        for df_chunk in chunk_generator:
            chunk_num += 1
            chunk_start = time.time()

            # Convert to records and enhance
            records = df_chunk.to_dict('records')

            # Handle NaN values
            import numpy as np
            for record in records:
                for key, value in list(record.items()):
                    if isinstance(value, np.ndarray):
                        record[key] = value.tolist()
                    elif isinstance(value, list):
                        pass  # Keep as list
                    elif pd.api.types.is_scalar(value):
                        try:
                            if pd.isna(value):
                                record[key] = None
                        except (ValueError, TypeError):
                            pass

            # Enhance records
            enhanced_data = []
            for record in records:
                if class_name == 'SystemProcess':
                    record = extract_provenance_info(record)
                record = add_computed_fields(record, class_name)
                enhanced_data.append(record)

            # Insert chunk
            collection.insert(enhanced_data)
            total_loaded += len(enhanced_data)

            chunk_time = time.time() - chunk_start

            # Progress update
            if num_chunks:
                progress_pct = (chunk_num / num_chunks) * 100
                print(f&quot;  [{chunk_num}/{num_chunks}] {progress_pct:5.1f}% - &quot;
                      f&quot;Loaded {len(enhanced_data):,} rows in {chunk_time:.1f}s &quot;
                      f&quot;(total: {total_loaded:,})&quot;)
            else:
                print(f&quot;  [Chunk {chunk_num}] Loaded {len(enhanced_data):,} rows &quot;
                      f&quot;in {chunk_time:.1f}s (total: {total_loaded:,})&quot;)

            # Force garbage collection after each chunk
            import gc
            gc.collect()

        elapsed = time.time() - start_time
        print(f&quot;  ‚úÖ Loaded {total_loaded:,} records in {elapsed:.1f}s &quot;
              f&quot;({total_loaded/elapsed:.0f} records/sec)&quot;)

        return total_loaded

    except Exception as e:
        print(f&quot;  ‚ùå Error loading data: {e}&quot;)
        if verbose:
            import traceback
            traceback.print_exc()
        return total_loaded  # Return partial count
</code></pre>
<h4 id="113-update-main-loading-function">1.1.3: Update main loading function</h4>
<p><strong>Location</strong>: Replace lines 502-548 (dynamic table loading section)</p>
<pre><code class="language-python"># Load dynamic tables (optional, chunked for large files)
if include_dynamic:
    print(f&quot;\n{'='*60}&quot;)
    print(f&quot;üì¶ Loading Dynamic Data Tables (ddt_*)&quot;)
    print(f&quot;{'='*60}&quot;)
    if max_dynamic_rows is not None:
        print(f&quot;‚ö†Ô∏è  Note: Dynamic tables sampled at {max_dynamic_rows:,} rows each&quot;)
    else:
        print(f&quot;‚ö†Ô∏è  Note: Loading complete brick data using CHUNKED mode&quot;)
        print(f&quot;   Memory-safe for machines with 64+ GB RAM&quot;)
    print(f&quot;   (Total: 82.6M rows across ~20 brick tables)&quot;)

    # Load ddt_ndarray (index table - small, load normally)
    ndarray_path = cdm_db_path / &quot;ddt_ndarray&quot;
    if ndarray_path.exists():
        class_name = TABLE_TO_CLASS[&quot;ddt_ndarray&quot;]
        count = load_parquet_collection(
            ndarray_path, class_name, db, schema_view,
            max_rows=None,  # Small table, load fully
            verbose=verbose
        )
        results[class_name] = count
        total_records += count

    # Load brick tables with chunking
    brick_tables = sorted([d for d in cdm_db_path.iterdir()
                   if d.is_dir() and d.name.startswith(&quot;ddt_brick&quot;)])

    if brick_tables:
        # Determine how many bricks to load
        bricks_to_load = len(brick_tables) if num_bricks is None else min(num_bricks, len(brick_tables))

        print(f&quot;\n  Found {len(brick_tables)} brick tables...&quot;)
        print(f&quot;  Loading {bricks_to_load} brick table(s)...&quot;)

        # Separate large bricks from small ones
        large_brick_threshold_mb = 50  # Bricks &gt;50 MB compressed use chunking
        large_bricks = []
        small_bricks = []

        for brick_path in brick_tables[:bricks_to_load]:
            brick_size_mb = sum(f.stat().st_size for f in brick_path.glob(&quot;*.parquet&quot;)) / (1024**2)
            if brick_size_mb &gt; large_brick_threshold_mb:
                large_bricks.append((brick_path, brick_size_mb))
            else:
                small_bricks.append((brick_path, brick_size_mb))

        print(f&quot;  ‚Ä¢ Small bricks (&lt;50 MB): {len(small_bricks)} (standard loading)&quot;)
        print(f&quot;  ‚Ä¢ Large bricks (‚â•50 MB): {len(large_bricks)} (chunked loading)&quot;)

        # Load small bricks first (faster, standard loading)
        for i, (brick_path, size_mb) in enumerate(small_bricks, 1):
            print(f&quot;\n  [Small {i}/{len(small_bricks)}] {brick_path.name} ({size_mb:.1f} MB)&quot;)
            count = load_parquet_collection(
                brick_path, &quot;DynamicDataArray&quot;, db, schema_view,
                max_rows=max_dynamic_rows,
                verbose=verbose
            )
            total_records += count

        # Load large bricks with chunking (memory-safe)
        for i, (brick_path, size_mb) in enumerate(large_bricks, 1):
            print(f&quot;\n  [Large {i}/{len(large_bricks)}] {brick_path.name} ({size_mb:.1f} MB)&quot;)
            count = load_parquet_collection_chunked(
                brick_path, &quot;DynamicDataArray&quot;, db, schema_view,
                max_rows=max_dynamic_rows,
                chunk_size=100_000,  # 100K rows per chunk
                verbose=verbose
            )
            total_records += count

        if len(brick_tables) &gt; bricks_to_load:
            print(f&quot;\n  ‚ö†Ô∏è  Skipped {len(brick_tables) - bricks_to_load} additional brick tables&quot;)
</code></pre>
<p><strong>Estimated Effort</strong>: 4 hours</p>
<hr />
<h3 id="task-12-update-just-commands-with-safe-defaults">Task 1.2: Update Just Commands with Safe Defaults</h3>
<p><strong>File</strong>: <code>project.justfile</code></p>
<p><strong>Changes Required</strong>:</p>
<h4 id="121-update-load-cdm-store-bricks-lines-284-295">1.2.1: Update load-cdm-store-bricks (lines 284-295)</h4>
<p><strong>Replace</strong> with:</p>
<pre><code class="language-justfile"># Load CDM parquet with brick tables (SAFE: sampled at 100K rows)
[group('CDM data management')]
load-cdm-store-bricks db='data/enigma_coral.db' output='cdm_store_bricks.db' num_bricks='20' max_rows='100000':
  @echo &quot;üì¶ Loading CDM parquet data (core + first {{num_bricks}} brick tables)...&quot;
  @echo &quot;‚ö†Ô∏è  SAFE MODE: Sampling {{max_rows}} rows per brick table&quot;
  @echo &quot;   (For full load, use: just load-cdm-store-bricks-full)&quot;
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --output {{output}} \
    --include-system \
    --include-static \
    --num-bricks {{num_bricks}} \
    --max-dynamic-rows {{max_rows}} \
    --create-indexes \
    --show-info \
    --verbose
  @echo &quot;‚úÖ Database ready: {{output}}&quot;
</code></pre>
<h4 id="122-add-new-load-cdm-store-bricks-full-command">1.2.2: Add new load-cdm-store-bricks-full command</h4>
<p><strong>Insert</strong> after line 295:</p>
<pre><code class="language-justfile"># Load CDM parquet with ALL brick tables (FULL: no sampling - requires 128+ GB RAM)
[group('CDM data management')]
load-cdm-store-bricks-full db='data/enigma_coral.db' output='cdm_store_bricks_full.db' num_bricks='20':
  @echo &quot;‚ö†Ô∏è  ============================================&quot;
  @echo &quot;‚ö†Ô∏è  WARNING: Full brick load requires ~100+ GB RAM&quot;
  @echo &quot;‚ö†Ô∏è  ============================================&quot;
  @echo &quot;&quot;
  @echo &quot;This will load ALL rows from {{num_bricks}} brick tables, including:&quot;
  @echo &quot;  ‚Ä¢ ddt_brick0000476: 320 million rows (383 MB ‚Üí ~7 GB in memory)&quot;
  @echo &quot;  ‚Ä¢ Other bricks: ~500K rows total&quot;
  @echo &quot;&quot;
  @echo &quot;Requirements:&quot;
  @echo &quot;  ‚Ä¢ RAM: 128 GB minimum (256 GB recommended)&quot;
  @echo &quot;  ‚Ä¢ Time: 30-60 minutes&quot;
  @echo &quot;  ‚Ä¢ Disk: 50+ GB free space&quot;
  @echo &quot;&quot;
  @echo &quot;Press Ctrl+C to cancel, or wait 10 seconds to continue...&quot;
  @sleep 10
  @echo &quot;&quot;
  @echo &quot;Starting full load with chunked processing (memory-safe)...&quot;
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --output {{output}} \
    --include-system \
    --include-static \
    --num-bricks {{num_bricks}} \
    --create-indexes \
    --show-info \
    --verbose
  @echo &quot;‚úÖ Database ready: {{output}}&quot;
</code></pre>
<h4 id="123-update-load-cdm-store-sample-lines-269-280">1.2.3: Update load-cdm-store-sample (lines 269-280)</h4>
<p><strong>Replace</strong> with:</p>
<pre><code class="language-justfile"># Load CDM parquet with core tables + first 5 brick tables (QUICK SAMPLE)
[group('CDM data management')]
load-cdm-store-sample db='data/enigma_coral.db' output='cdm_store_sample.db' num_bricks='5' max_rows='10000':
  @echo &quot;üì¶ Loading CDM parquet data (QUICK SAMPLE: first {{num_bricks}} bricks, {{max_rows}} rows each)...&quot;
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --output {{output}} \
    --include-system \
    --include-static \
    --num-bricks {{num_bricks}} \
    --max-dynamic-rows {{max_rows}} \
    --create-indexes \
    --show-info \
    --verbose
  @echo &quot;‚úÖ Database ready: {{output}}&quot;
</code></pre>
<p><strong>Estimated Effort</strong>: 30 minutes</p>
<hr />
<h2 id="phase-2-user-experience-p1-warnings-progress">Phase 2: User Experience (P1 - Warnings &amp; Progress)</h2>
<h3 id="task-21-add-memory-monitoring">Task 2.1: Add Memory Monitoring</h3>
<p><strong>File</strong>: <code>scripts/cdm_analysis/load_cdm_parquet_to_store.py</code></p>
<p><strong>Location</strong>: Add at top of file (after imports)</p>
<pre><code class="language-python">import psutil

def get_memory_info() -&gt; Dict[str, float]:
    &quot;&quot;&quot;Get current system memory information in GB.&quot;&quot;&quot;
    mem = psutil.virtual_memory()
    return {
        'total_gb': mem.total / (1024**3),
        'available_gb': mem.available / (1024**3),
        'used_gb': mem.used / (1024**3),
        'percent': mem.percent
    }

def estimate_memory_requirement(parquet_path: Path) -&gt; float:
    &quot;&quot;&quot;
    Estimate memory required to load parquet file in GB.

    Args:
        parquet_path: Path to parquet file or directory

    Returns:
        Estimated memory in GB
    &quot;&quot;&quot;
    if parquet_path.is_dir():
        total_size = sum(f.stat().st_size for f in parquet_path.glob(&quot;*.parquet&quot;))
    else:
        total_size = parquet_path.stat().st_size

    # Estimate: compressed_size √ó 8 (decompression) √ó 2 (processing overhead)
    estimated_gb = (total_size / (1024**3)) * 16
    return estimated_gb

def check_memory_warning(parquet_path: Path, verbose: bool = False) -&gt; bool:
    &quot;&quot;&quot;
    Check if sufficient memory is available and warn if low.

    Args:
        parquet_path: Path to parquet file/directory
        verbose: Print detailed memory info

    Returns:
        True if sufficient memory, False otherwise
    &quot;&quot;&quot;
    mem_info = get_memory_info()
    required_gb = estimate_memory_requirement(parquet_path)

    if verbose or required_gb &gt; mem_info['available_gb'] * 0.5:
        print(f&quot;\n  üíæ Memory Check:&quot;)
        print(f&quot;     System Total: {mem_info['total_gb']:.1f} GB&quot;)
        print(f&quot;     Available: {mem_info['available_gb']:.1f} GB&quot;)
        print(f&quot;     Estimated Required: {required_gb:.1f} GB&quot;)

    if required_gb &gt; mem_info['available_gb']:
        print(f&quot;\n  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  MEMORY WARNING ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è&quot;)
        print(f&quot;  This file may cause out-of-memory errors!&quot;)
        print(f&quot;  Required: ~{required_gb:.1f} GB&quot;)
        print(f&quot;  Available: {mem_info['available_gb']:.1f} GB&quot;)
        print(f&quot;\n  Recommendations:&quot;)
        print(f&quot;    1. Use --max-dynamic-rows to limit memory (e.g., --max-dynamic-rows 100000)&quot;)
        print(f&quot;    2. Close other applications to free memory&quot;)
        print(f&quot;    3. Use a machine with more RAM (128+ GB recommended)&quot;)
        print(f&quot;&quot;)
        return False

    return True
</code></pre>
<p><strong>Location</strong>: Update <code>load_parquet_collection_chunked</code> (add memory check)</p>
<p>Insert after line where <code>table_name = parquet_path.name</code>:</p>
<pre><code class="language-python"># Check memory availability
if not check_memory_warning(parquet_path, verbose=verbose):
    print(f&quot;  ‚ö†Ô∏è  Proceeding with chunked loading (memory-safe)...&quot;)
</code></pre>
<p><strong>Estimated Effort</strong>: 2 hours</p>
<hr />
<h3 id="task-22-add-progress-bars">Task 2.2: Add Progress Bars</h3>
<p><strong>File</strong>: <code>scripts/cdm_analysis/load_cdm_parquet_to_store.py</code></p>
<p><strong>Requirements</strong>: Add <code>tqdm</code> to dependencies</p>
<pre><code class="language-bash">uv pip install tqdm
</code></pre>
<p><strong>Changes</strong>:</p>
<pre><code class="language-python">from tqdm import tqdm

def load_parquet_collection_chunked(
    parquet_path: Path,
    class_name: str,
    db,
    schema_view: SchemaView,
    max_rows: Optional[int] = None,
    chunk_size: int = 100_000,
    verbose: bool = False,
    show_progress: bool = True  # NEW parameter
) -&gt; int:
    &quot;&quot;&quot;...&quot;&quot;&quot;

    # ... existing code ...

    # Create progress bar
    if show_progress and total_rows:
        pbar = tqdm(
            total=load_rows,
            desc=f&quot;Loading {table_name}&quot;,
            unit=&quot;rows&quot;,
            unit_scale=True,
            bar_format=&quot;{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]&quot;
        )
    else:
        pbar = None

    try:
        chunk_generator = read_parquet_chunked(...)

        for df_chunk in chunk_generator:
            # ... existing chunk processing ...

            # Update progress bar
            if pbar:
                pbar.update(len(enhanced_data))
            elif not verbose:  # Print simple progress if no bar
                print(f&quot;  [{chunk_num}/{num_chunks}] {total_loaded:,} rows loaded&quot;, end='\r')

            # ... existing code ...

        if pbar:
            pbar.close()

    except Exception as e:
        if pbar:
            pbar.close()
        # ... existing error handling ...
</code></pre>
<p><strong>Estimated Effort</strong>: 1 hour</p>
<hr />
<h2 id="phase-3-performance-optimization-p2-optional">Phase 3: Performance Optimization (P2 - Optional)</h2>
<h3 id="task-31-direct-duckdb-import-advanced">Task 3.1: Direct DuckDB Import (Advanced)</h3>
<p><strong>Status</strong>: Optional - requires linkml-store API changes</p>
<p><strong>Approach</strong>: Bypass pandas entirely and use DuckDB's native parquet reader</p>
<pre><code class="language-python">def load_parquet_to_duckdb_native(
    parquet_path: Path,
    collection_name: str,
    db,
    max_rows: Optional[int] = None
) -&gt; int:
    &quot;&quot;&quot;
    Load parquet directly into DuckDB without pandas.

    This is 10-50x faster and uses minimal memory.
    &quot;&quot;&quot;
    import duckdb

    # Get DuckDB connection
    conn = db._get_duckdb_connection()  # May need linkml-store API update

    # Build query
    parquet_pattern = f&quot;{parquet_path}/**/*.parquet&quot;

    if max_rows:
        query = f&quot;&quot;&quot;
            CREATE TABLE {collection_name} AS
            SELECT * FROM read_parquet('{parquet_pattern}')
            LIMIT {max_rows}
        &quot;&quot;&quot;
    else:
        query = f&quot;&quot;&quot;
            CREATE TABLE {collection_name} AS
            SELECT * FROM read_parquet('{parquet_pattern}')
        &quot;&quot;&quot;

    # Execute (streaming, no memory overhead)
    conn.execute(query)

    # Get count
    count = conn.execute(f&quot;SELECT COUNT(*) FROM {collection_name}&quot;).fetchone()[0]

    return count
</code></pre>
<p><strong>Estimated Effort</strong>: 8-16 hours (requires linkml-store modifications)</p>
<hr />
<h2 id="testing-plan">Testing Plan</h2>
<h3 id="unit-tests">Unit Tests</h3>
<p><strong>File</strong>: <code>tests/test_brick_loading.py</code> (new file)</p>
<pre><code class="language-python">import pytest
from pathlib import Path
from scripts.cdm_analysis.load_cdm_parquet_to_store import (
    read_parquet_chunked,
    estimate_memory_requirement,
    get_memory_info
)

def test_chunked_reading_small_file():
    &quot;&quot;&quot;Test chunked reading with small parquet file.&quot;&quot;&quot;
    path = Path(&quot;data/enigma_coral.db/ddt_brick0000072&quot;)
    chunks = list(read_parquet_chunked(path, chunk_size=100, max_rows=500))

    assert len(chunks) &gt;= 1
    assert sum(len(chunk) for chunk in chunks) &lt;= 500

def test_chunked_reading_max_rows():
    &quot;&quot;&quot;Test max_rows parameter limits total rows.&quot;&quot;&quot;
    path = Path(&quot;data/enigma_coral.db/ddt_brick0000452&quot;)
    chunks = list(read_parquet_chunked(path, chunk_size=1000, max_rows=5000))

    total_rows = sum(len(chunk) for chunk in chunks)
    assert total_rows == 5000

def test_memory_estimation():
    &quot;&quot;&quot;Test memory estimation is reasonable.&quot;&quot;&quot;
    path = Path(&quot;data/enigma_coral.db/ddt_brick0000476&quot;)
    estimated = estimate_memory_requirement(path)

    # Should estimate 3-10 GB for 383 MB compressed file
    assert 3.0 &lt;= estimated &lt;= 10.0

def test_memory_info():
    &quot;&quot;&quot;Test memory info retrieval.&quot;&quot;&quot;
    mem = get_memory_info()

    assert mem['total_gb'] &gt; 0
    assert mem['available_gb'] &gt; 0
    assert 0 &lt;= mem['percent'] &lt;= 100
</code></pre>
<h3 id="integration-tests">Integration Tests</h3>
<p><strong>Scenario 1: Small machine (16 GB)</strong></p>
<pre><code class="language-bash"># Should succeed with sampling
just load-cdm-store-sample data/enigma_coral.db test_16gb.db

# Expected: 2-5 minutes, 2-4 GB peak memory
</code></pre>
<p><strong>Scenario 2: Medium machine (64 GB)</strong></p>
<pre><code class="language-bash"># Should succeed with default sampling (100K rows)
just load-cdm-store-bricks data/enigma_coral.db test_64gb.db

# Expected: 10-15 minutes, 8-12 GB peak memory

# Should succeed with full load (chunked)
just load-cdm-store-bricks-full data/enigma_coral.db test_64gb_full.db

# Expected: 30-45 minutes, 60-80 GB peak memory
</code></pre>
<p><strong>Scenario 3: Large machine (128+ GB)</strong></p>
<pre><code class="language-bash"># Should succeed with full load
just load-cdm-store-bricks-full data/enigma_coral.db test_128gb.db

# Expected: 25-35 minutes, 80-100 GB peak memory
</code></pre>
<h3 id="performance-benchmarks">Performance Benchmarks</h3>
<p>Track improvements over baseline:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Before</th>
<th>After (P0)</th>
<th>After (P2)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Peak (64GB machine)</td>
<td>&gt;100 GB (OOM)</td>
<td>12 GB</td>
<td>8 GB</td>
</tr>
<tr>
<td>Load Time (all bricks, sampled)</td>
<td>N/A</td>
<td>15 min</td>
<td>10 min</td>
</tr>
<tr>
<td>Load Time (all bricks, full)</td>
<td>N/A</td>
<td>40 min</td>
<td>25 min</td>
</tr>
<tr>
<td>Success Rate (64GB)</td>
<td>0%</td>
<td>100%</td>
<td>100%</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rollout-plan">Rollout Plan</h2>
<h3 id="week-1-critical-fixes-p0">Week 1: Critical Fixes (P0)</h3>
<p><strong>Days 1-2</strong>: Implement chunked loading
- [ ] Add <code>read_parquet_chunked</code> function
- [ ] Add <code>load_parquet_collection_chunked</code> function
- [ ] Update <code>load_all_cdm_parquet</code> to use chunking
- [ ] Test on brick0000476</p>
<p><strong>Day 3</strong>: Update commands and documentation
- [ ] Update <code>project.justfile</code> with safe defaults
- [ ] Add <code>load-cdm-store-bricks-full</code> command
- [ ] Update README with memory requirements
- [ ] Test all just commands</p>
<h3 id="week-2-user-experience-p1">Week 2: User Experience (P1)</h3>
<p><strong>Day 4</strong>: Memory monitoring
- [ ] Add <code>psutil</code> dependency
- [ ] Implement memory checking functions
- [ ] Add warnings for low memory
- [ ] Test on 16GB, 64GB, 128GB machines</p>
<p><strong>Day 5</strong>: Progress indicators
- [ ] Add <code>tqdm</code> dependency
- [ ] Implement progress bars
- [ ] Test user experience</p>
<h3 id="week-3-optional-performance-p2">Week 3+: Optional Performance (P2)</h3>
<p><strong>If time permits</strong>:
- [ ] Research linkml-store DuckDB API
- [ ] Implement direct DuckDB import
- [ ] Benchmark performance gains</p>
<hr />
<h2 id="success-criteria">Success Criteria</h2>
<h3 id="must-have-p0">Must Have (P0)</h3>
<ul>
<li>‚úÖ Loading brick0000476 succeeds on 64GB machine</li>
<li>‚úÖ Peak memory stays under 80GB for full load</li>
<li>‚úÖ No OOM errors on 64GB+ machines</li>
<li>‚úÖ Just commands have safe defaults (sampling)</li>
</ul>
<h3 id="should-have-p1">Should Have (P1)</h3>
<ul>
<li>‚úÖ Memory warnings displayed before large loads</li>
<li>‚úÖ Progress bars show load status</li>
<li>‚úÖ Updated documentation with requirements</li>
<li>‚úÖ All tests passing</li>
</ul>
<h3 id="nice-to-have-p2">Nice to Have (P2)</h3>
<ul>
<li>üî≤ Direct DuckDB import (10x+ faster)</li>
<li>üî≤ Automatic chunk size tuning</li>
<li>üî≤ Parallel chunk loading</li>
</ul>
<hr />
<h2 id="risk-mitigation">Risk Mitigation</h2>
<h3 id="risk-1-chunked-loading-is-slower">Risk 1: Chunked loading is slower</h3>
<p><strong>Mitigation</strong>: Benchmark shows ~40% slower (15 min vs 10 min), but this is acceptable to avoid OOM. For users with 128+ GB RAM, they can still use full non-chunked loading.</p>
<h3 id="risk-2-breaking-changes-for-existing-scripts">Risk 2: Breaking changes for existing scripts</h3>
<p><strong>Mitigation</strong>: Keep old functions, add new chunked variants. Update just commands to use new functions, but allow <code>--no-chunking</code> flag for backwards compatibility.</p>
<h3 id="risk-3-linkml-store-compatibility-issues">Risk 3: linkml-store compatibility issues</h3>
<p><strong>Mitigation</strong>: Test thoroughly with linkml-store 0.1.x. Chunked inserts are standard operations and should work reliably.</p>
<hr />
<h2 id="documentation-updates">Documentation Updates</h2>
<h3 id="files-to-update">Files to Update</h3>
<ol>
<li><strong>README.md</strong>:</li>
<li>Add memory requirements section</li>
<li>Update quick start examples</li>
<li>
<p>Add troubleshooting for OOM</p>
</li>
<li>
<p><strong>docs/CDM_PARQUET_STORE_GUIDE.md</strong>:</p>
</li>
<li>Document chunked loading</li>
<li>Add memory requirements table</li>
<li>
<p>Add performance benchmarks</p>
</li>
<li>
<p><strong>scripts/cdm_analysis/load_cdm_parquet_to_store.py</strong>:</p>
</li>
<li>Update docstring with memory requirements</li>
<li>
<p>Add examples for chunked loading</p>
</li>
<li>
<p><strong>project.justfile</strong>:</p>
</li>
<li>Update command help text</li>
<li>Add warnings to full load commands</li>
</ol>
<hr />
<h2 id="appendix-code-review-checklist">Appendix: Code Review Checklist</h2>
<p>Before merging:</p>
<ul>
<li>[ ] All new functions have docstrings</li>
<li>[ ] Memory estimation is conservative (doesn't underestimate)</li>
<li>[ ] Garbage collection is called after each chunk</li>
<li>[ ] Progress reporting works in both verbose and quiet modes</li>
<li>[ ] Error handling preserves partial progress</li>
<li>[ ] Tests cover edge cases (empty files, single row, max_rows boundary)</li>
<li>[ ] Just commands have clear warnings</li>
<li>[ ] README is updated with memory requirements</li>
<li>[ ] No breaking changes to existing APIs</li>
</ul>
<hr />
<p><strong>Plan Created</strong>: 2026-01-23
<strong>Estimated Effort</strong>: 12-16 hours development + 4-6 hours testing
<strong>Target Completion</strong>: 1-2 weeks
<strong>Status</strong>: Ready for implementation approval</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["content.tabs.link"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>