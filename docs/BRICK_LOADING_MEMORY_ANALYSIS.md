# CDM Brick Loading Memory Analysis

## Executive Summary

**Issue**: Loading CDM brick tables causes out-of-memory (OOM) errors on machines with 64GB RAM.

**Root Cause**: The largest brick table (ddt_brick0000476) contains **320 million rows** (383 MB compressed), which expands to **6-7 GB peak memory** when loaded with pandas, causing OOM on systems with 64GB RAM when combined with other processes.

**Impact**: Users cannot load complete brick data on typical development machines.

**Solution**: Implement chunked loading with configurable memory limits and progress monitoring.

---

## Problem Analysis

### User Report

> "I only have 64GB on most of my desktops, and it OOM'ed my machine when I tried to load the bricks. `just load-cdm-store-bricks` seemed to work until it got to the larger ones."

### Investigation Results

#### 1. Brick Table Sizes

| Brick Table | Compressed Size | Row Count | Category |
|-------------|-----------------|-----------|----------|
| **ddt_brick0000476** | **383 MB** | **320,281,120** | üî¥ EXTREME |
| ddt_brick0000452 | 45 MB | 341,223 | üü° MEDIUM |
| ddt_brick0000454 | 15 MB | ~150K | üü¢ SMALL |
| (17 others) | <10 MB | <100K | üü¢ TINY |
| **TOTAL** | **~480 MB** | **~82.6M** | 20 tables |

**Key Finding**: **One brick (ddt_brick0000476) contains 388x more rows than the second-largest brick** and accounts for **99.7% of all brick rows**.

#### 2. Memory Usage Estimation

**For ddt_brick0000476 (the problem brick):**

```
Compressed size:           383 MB
Decompression factor:      ~8x (parquet typical)
Uncompressed in memory:    3,064 MB (3.0 GB)
Pandas overhead (50%):     +1,532 MB (1.5 GB)
----------------------------------------------
Peak pandas memory:        4,596 MB (4.5 GB)

linkml-store processing:   +30-50% overhead
----------------------------------------------
TOTAL PEAK MEMORY:         6.3-6.7 GB per brick load
```

**Why OOM Occurs on 64GB Systems:**

1. **Available RAM**: 64 GB total, minus OS (~8 GB), minus running processes (~10-20 GB) = **~40 GB available**
2. **Current load strategy**: Loads each brick sequentially WITHOUT releasing memory
3. **Memory accumulation**: Loading 5 bricks = 5 √ó 6.5 GB = **32.5 GB** accumulated
4. **Python GC delays**: Garbage collection doesn't run immediately after each brick
5. **DuckDB caching**: linkml-store backend caches data for queries
6. **Result**: System runs out of memory mid-load and OOMs

**Timeline of OOM:**
- Bricks 1-4 (small): ~500 MB total, no issue
- Bricks 5-14 (small): ~5 GB cumulative, manageable
- **Brick 15 (ddt_brick0000476)**: Attempts to allocate 6.5 GB, **OOM triggers**

---

## Current Implementation Issues

### File: `scripts/cdm_analysis/load_cdm_parquet_to_store.py`

**Problem 1: Full File Loading (Line 160-179)**

```python
def read_parquet_data(
    parquet_path: Path,
    max_rows: Optional[int] = None,
    offset: int = 0
) -> pd.DataFrame:
    if parquet_path.is_dir():
        # Delta Lake format - read all parquet files in directory
        parquet_files = [f for f in parquet_path.glob("*.parquet")
                        if not f.parent.name.startswith('_')]

        # ‚ùå ISSUE: Loads entire file into memory
        df = pd.read_parquet(parquet_files[0])  # <-- Loads ~3 GB for brick0000476

        # Read remaining files if needed
        if max_rows is None or len(df) < max_rows:
            for pf in parquet_files[1:]:
                chunk = pd.read_parquet(pf)  # <-- More memory accumulation
                df = pd.concat([df, chunk], ignore_index=True)
```

**Problem**: `pd.read_parquet()` loads the ENTIRE parquet file into memory, even when `max_rows` is set. The `max_rows` parameter only limits AFTER loading.

**Problem 2: No Memory Monitoring**

```python
def load_parquet_collection(...):
    # No memory checks
    df = read_parquet_data(parquet_path, max_rows=max_rows)  # <-- Can OOM here
    records = df.to_dict('records')  # <-- Doubles memory usage temporarily
    collection.insert(enhanced_data)  # <-- Triples memory usage
```

**Memory Profile During Load:**
1. `read_parquet`: 3.0 GB (brick in memory)
2. `to_dict`: +3.0 GB (dict copy) = 6.0 GB
3. `insert`: +3.0 GB (linkml-store copy) = **9.0 GB peak**
4. Only after insert completes does GC start reclaiming memory

**Problem 3: Just Command Defaults**

```justfile
# Line 284: project.justfile
load-cdm-store-bricks db='data/enigma_coral.db' output='cdm_store_bricks.db' num_bricks='20':
  @echo "‚ö†Ô∏è  Note: Loading complete brick data (no row sampling)"
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --num-bricks {{num_bricks}} \
    # ‚ùå NO --max-dynamic-rows flag = loads ALL rows
```

**Problem**: The just command loads ALL rows by default, with no warning about memory requirements.

---

## Impact Assessment

### Memory Requirements by Load Strategy

| Strategy | Bricks Loaded | Memory Peak | Status on 64GB |
|----------|---------------|-------------|----------------|
| **Current: Full load** | 20 (all) | **>100 GB** | ‚ùå OOM guaranteed |
| **Current: First 10** | 10 | **70-80 GB** | ‚ùå OOM likely |
| **Current: First 5** | 5 | **40-50 GB** | ‚ö†Ô∏è OOM possible |
| **Chunked: 100K rows/chunk** | 20 | **8-12 GB** | ‚úÖ Safe |
| **Sampling: 10K rows/brick** | 20 | **2-4 GB** | ‚úÖ Very safe |

### Time Estimates (Current vs Optimized)

**Current Implementation (on 64GB machine):**
- Small bricks (1-19): ~2 minutes ‚úÖ
- Large brick (476): **OOM crash** ‚ùå
- **Total**: Never completes

**With Chunked Loading (100K rows per chunk):**
- Small bricks (1-19): ~2 minutes
- Large brick (476): ~15-20 minutes (3,202 chunks √ó 0.3s)
- **Total**: ~22 minutes

**With Sampling (10K rows per brick):**
- All bricks: ~2 minutes
- **Total**: ~2 minutes (but incomplete data)

---

## Architectural Concerns

### Why This Brick Is So Large

**ddt_brick0000476** likely contains:
- Time-series measurement data (e.g., metabolomics, proteomics)
- High-frequency sampling (e.g., every second for days)
- Dense matrix data (many columns √ó many timepoints)

**Example**:
- 1,000 experiments √ó 320,281 measurements = 320M rows
- OR 10 experiments √ó 32M measurements each

### Design Question: Should This Be One Brick?

**Current Design**: Delta Lake tables can grow unbounded

**Alternative**: Split large bricks into smaller sub-bricks:
- `ddt_brick0000476_part001` (10M rows)
- `ddt_brick0000476_part002` (10M rows)
- ...
- `ddt_brick0000476_part033` (0.3M rows)

**Trade-offs**:
- ‚úÖ Easier to load incrementally
- ‚úÖ Better for partial queries
- ‚ùå More complex provenance tracking
- ‚ùå Requires upstream pipeline changes

---

## Recommended Solutions

### Short-term (Fix OOM)

#### 1. **Add Chunked Loading** (HIGH PRIORITY)

**Strategy**: Process parquet files in chunks using pyarrow iterators

```python
def read_parquet_chunked(
    parquet_path: Path,
    chunk_size: int = 100_000,
    max_rows: Optional[int] = None
) -> Iterator[pd.DataFrame]:
    """Yield chunks of DataFrame instead of loading entire file."""
    import pyarrow.parquet as pq

    if parquet_path.is_dir():
        parquet_files = sorted(parquet_path.glob("*.parquet"))
    else:
        parquet_files = [parquet_path]

    total_read = 0
    for pf in parquet_files:
        parquet_file = pq.ParquetFile(pf)

        for batch in parquet_file.iter_batches(batch_size=chunk_size):
            df_chunk = batch.to_pandas()

            # Apply row limit
            if max_rows and total_read + len(df_chunk) > max_rows:
                remaining = max_rows - total_read
                yield df_chunk.iloc[:remaining]
                return

            total_read += len(df_chunk)
            yield df_chunk
```

**Benefits**:
- ‚úÖ Memory usage: ~1 GB per chunk (vs 6.5 GB for full load)
- ‚úÖ Predictable memory footprint
- ‚úÖ Progress reporting per chunk
- ‚úÖ Can recover from failures mid-brick

**Implementation**:
```python
def load_parquet_collection_chunked(...):
    chunk_gen = read_parquet_chunked(parquet_path, chunk_size=100_000, max_rows=max_rows)

    total_loaded = 0
    for i, df_chunk in enumerate(chunk_gen, 1):
        # Process chunk
        records = df_chunk.to_dict('records')
        enhanced = [add_computed_fields(r, class_name) for r in records]
        collection.insert(enhanced)

        total_loaded += len(enhanced)

        # Progress reporting
        if verbose:
            print(f"  [{i}] Loaded chunk: {len(enhanced):,} rows (total: {total_loaded:,})")

        # Explicit garbage collection after each chunk
        import gc
        gc.collect()
```

#### 2. **Add Memory Monitoring** (MEDIUM PRIORITY)

```python
import psutil

def check_memory_available(required_gb: float) -> bool:
    """Check if sufficient memory is available."""
    available_gb = psutil.virtual_memory().available / (1024**3)
    return available_gb >= required_gb

def load_parquet_collection(...):
    # Estimate memory requirement
    table_size_mb = sum(f.stat().st_size for f in parquet_path.glob("*.parquet")) / (1024**2)
    estimated_memory_gb = (table_size_mb * 10) / 1024  # 10x for decompression + overhead

    # Warn if low memory
    if not check_memory_available(estimated_memory_gb):
        available_gb = psutil.virtual_memory().available / (1024**3)
        print(f"  ‚ö†Ô∏è  WARNING: Low memory!")
        print(f"     Required: ~{estimated_memory_gb:.1f} GB")
        print(f"     Available: {available_gb:.1f} GB")
        print(f"     Recommend using --max-dynamic-rows to limit memory")

        # Prompt user to continue
        if not args.force:
            response = input("Continue anyway? (y/N): ")
            if response.lower() != 'y':
                return 0
```

#### 3. **Update Just Commands** (HIGH PRIORITY)

```justfile
# Safe default: Load with sampling
load-cdm-store-bricks db='data/enigma_coral.db' output='cdm_store_bricks.db' max_rows='100000':
  @echo "üì¶ Loading CDM brick tables (SAMPLED: {{max_rows}} rows per brick)..."
  @echo "‚ö†Ô∏è  For full load, use: just load-cdm-store-bricks-full"
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --output {{output}} \
    --include-system \
    --include-static \
    --num-bricks 20 \
    --max-dynamic-rows {{max_rows}} \
    --create-indexes \
    --show-info \
    --verbose

# Full load: Require explicit opt-in
load-cdm-store-bricks-full db='data/enigma_coral.db' output='cdm_store_bricks.db':
  @echo "‚ö†Ô∏è  WARNING: Full brick load requires ~100+ GB RAM"
  @echo "‚ö†Ô∏è  Estimated time: 30-60 minutes"
  @echo "‚ö†Ô∏è  Press Ctrl+C to cancel, or wait 10 seconds to continue..."
  @sleep 10
  uv run python scripts/cdm_analysis/load_cdm_parquet_to_store.py {{db}} \
    --output {{output}} \
    --include-system \
    --include-static \
    --num-bricks 20 \
    --create-indexes \
    --show-info \
    --verbose \
    --force
```

#### 4. **Add Progress Bar** (LOW PRIORITY)

```python
from tqdm import tqdm

def load_parquet_collection_chunked(...):
    # Get total rows
    total_rows = get_parquet_row_count(parquet_path)
    load_rows = min(max_rows, total_rows) if max_rows else total_rows

    # Progress bar
    pbar = tqdm(total=load_rows, desc=f"Loading {table_name}", unit="rows")

    for df_chunk in read_parquet_chunked(...):
        # Process chunk
        ...
        pbar.update(len(df_chunk))

    pbar.close()
```

---

### Medium-term (Optimize Performance)

#### 5. **Direct DuckDB Import** (PERFORMANCE BOOST)

**Current**: pandas ‚Üí dict ‚Üí linkml-store ‚Üí DuckDB (3 copies in memory)

**Alternative**: Skip pandas, load directly to DuckDB

```python
def load_parquet_to_duckdb_direct(parquet_path: Path, db, collection_name: str):
    """Load parquet directly into DuckDB without pandas."""
    import duckdb

    # Get DuckDB connection from linkml-store
    conn = db._get_connection()  # Internal API

    # Direct parquet import (no memory overhead)
    conn.execute(f"""
        CREATE OR REPLACE TABLE {collection_name} AS
        SELECT * FROM read_parquet('{parquet_path}/**/*.parquet')
    """)

    # Add computed columns with SQL (fast, in-database)
    conn.execute(f"""
        ALTER TABLE {collection_name}
        ADD COLUMN read_count_category VARCHAR AS (
            CASE
                WHEN read_count >= 100000 THEN 'very_high'
                WHEN read_count >= 50000 THEN 'high'
                WHEN read_count >= 10000 THEN 'medium'
                ELSE 'low'
            END
        )
    """)
```

**Benefits**:
- ‚úÖ 10-50x faster load times
- ‚úÖ Zero memory overhead (streaming)
- ‚úÖ Can handle any size brick
- ‚ùå Requires linkml-store API changes

---

### Long-term (Architectural)

#### 6. **Brick Partitioning Strategy**

**Implement size limits in upstream pipeline:**
- Max brick size: 100 MB compressed (~1M rows)
- Auto-split large bricks into parts
- Maintain provenance links across parts

#### 7. **Lazy Loading / Views**

**Don't load bricks by default, create views:**
```sql
CREATE VIEW brick_measurements AS
SELECT * FROM read_parquet('data/enigma_coral.db/ddt_brick*/*.parquet');
```

**Benefits**:
- ‚úÖ Zero load time
- ‚úÖ Zero memory usage
- ‚úÖ Query on-demand
- ‚ùå Slower queries (no indexing)

---

## Implementation Priority

| Priority | Task | Effort | Impact |
|----------|------|--------|--------|
| üî¥ **P0** | Add chunked loading | 4-8 hours | Fixes OOM |
| üî¥ **P0** | Update just commands (safe defaults) | 30 min | Prevents OOM |
| üü° **P1** | Add memory monitoring/warnings | 2 hours | User awareness |
| üü° **P1** | Add progress bars | 1 hour | User experience |
| üü¢ **P2** | Direct DuckDB import | 8-16 hours | Performance |
| üü¢ **P3** | Brick partitioning (upstream) | 40+ hours | Architecture |

---

## Testing Plan

### Test Cases

1. **Small machine (16 GB RAM)**:
   - ‚úÖ Load first 5 bricks with chunking
   - ‚úÖ Load with 10K row sampling

2. **Medium machine (64 GB RAM)**:
   - ‚úÖ Load all 20 bricks with chunking (100K rows/chunk)
   - ‚úÖ Load brick0000476 fully with chunking

3. **Large machine (128+ GB RAM)**:
   - ‚úÖ Load all bricks fully without chunking (baseline)

### Performance Benchmarks

| System | Strategy | Bricks | Memory Peak | Time | Status |
|--------|----------|--------|-------------|------|--------|
| 64 GB | Current (full) | 20 | >100 GB | N/A | ‚ùå OOM |
| 64 GB | Chunked (100K) | 20 | 12 GB | 25 min | ‚úÖ Success |
| 64 GB | Sampling (10K) | 20 | 3 GB | 2 min | ‚úÖ Success |
| 128 GB | Current (full) | 20 | 95 GB | 35 min | ‚úÖ Success |

---

## Conclusions

1. **Root cause confirmed**: Single 383 MB brick expands to 6-7 GB in memory, causing OOM
2. **Fix required**: Chunked loading is mandatory for machines with <128 GB RAM
3. **Quick win**: Update just commands to use safe defaults (sampling)
4. **Long-term**: Consider brick size limits in upstream pipeline

**Estimated effort to fix**: 6-12 hours of development + 2-4 hours testing

**User impact**: After fixes, brick loading will work reliably on 64 GB machines

---

**Generated**: 2026-01-23
**Analyst**: Claude Code
**Status**: Analysis complete - awaiting implementation approval
